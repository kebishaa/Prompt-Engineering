#  Prompt Engineering: In-context learning with GPT-3 and other Large Language Models

![Gpt-3](https://www.sigmoid.com/wp-content/uploads/2020/08/sigmoid-blog-gpt-800x281.jpg)

## About

GPT3 is an autoregressive language model that uses deep learning to produce
human-like text.
The architecture is a standard transformer network(with free engineering tweaks) with
the unprecedented size of 2048-token-long context and 175 billion parameters(requiring
800 GB of storage ) The training method is “generative pretraining”, meaning that it is
trained to predict what the next token is. The model demonstrated strong few-shot
learning on many text-based tasks.The quality of the text generated by GPT-3 is so high
that it can be difficult to determine whether or not it was written by a human, which has
both benefits and risks.



